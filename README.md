# üõ∞Ô∏è Energy-Efficient Ground Station Selection for LEO Downlink  
### RL Baselines: Random / Greedy / DQN (TLE-driven OCO-2 Simulation)

[![KICS 2025](https://img.shields.io/badge/Paper-KICS%202025-blue)](https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE12132784)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-green.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository implements a **minute-level LEO downlink scheduling simulator** using **Skyfield + TLE (OCO-2 orbit)** and compares baseline policies (**Random / Distance-Greedy / DQN**) for a multi-objective decision problem balancing:

- **Freshness** via **AoI (Age of Information)**
- **Queue stability** under finite capacity and overflow risk
- **Energy sustainability** with **distance-dependent transmission cost**

> **Core question:** Given time-varying visibility and distance-dependent transmission cost, which ground station should a satellite downlink to at each minute to balance freshness (AoI), queue stability, and energy sustainability?

---
## üìä Key Results (Evaluation)

Evaluation is conducted under **pure exploitation** for DQN (**Œµ = 0**) across **N evaluation episodes**, with **identical environment settings** for all policies.

> **Reproducibility note**  
> All reported metrics are computed from CSV logs generated by `eval.py`.  
> See: `random_log.csv`, `greedy_log.csv`, `dqn_best_log.csv`, and step-level logs (`*_steps_log.csv`, `*_queue_log.csv`).

### 1) Comparative Performance Summary

The table below reports **average performance over all evaluation episodes**.

| Metric | **Random** | **Distance-Greedy** | **DQN (Ours)** | Unit / Notes |
|:---|:---:|:---:|:---:|:---|
| **Mean Return** | -760.12 ¬± 29.5 | 281.18 ¬± 0.3 | **1304.74 ¬± 11.2** | Higher is better |
| **Avg. AoI (GS)** | ~712.4 | ~548.2 | **~124.6** | Minutes (lower is better) |
| **Avg. Battery Level** | 0.91 | 0.96 | **0.87** | Normalized (higher is better) |
| **Queue Dispersion** | 38.42 | 24.15 | **12.58** | Std across 10 GS (lower is better) |


### 2) Metric Interpretation

- **Mean Return**: The cumulative scalar reward reflecting the agent's ability to balance data freshness, load balancing, and energy conservation. 
- **Average AoI (GS)**:  Represents the average "age" of data across all 10 ground stations. 
- **Queue Dispersion**: The standard deviation of data stored in GS queues. Lower dispersion signifies successful load balancing, preventing localized bottlenecks.
- **Average Battery Level**: The normalized remaining energy of the satellite. It evaluates the operational sustainability of the downlink policy under power constraints.

### 3) Baseline Behavior Analysis

#### Random Policy (Blind Baseline)
- **Logic:** Randomly selects an action (0‚Äì10) at every timestep, ignoring visibility and energy constraints.
- **Observed Behavior:**  
  Serves as a lower bound. Exhibits high AoI and frequent penalties due to infeasible downlink attempts.

#### Distance-based Greedy Policy (Heuristic)
- **Logic:** Selects the closest visible Ground Station based on Euclidean distance.
- **Observed Behavior:**  
  Achieves substantial AoI reduction but behaves myopically, often over-communicating with nearby stations and rapidly depleting battery energy.

#### DQN Agent (Learned Optimization)
- **Logic:** Observes the full system state (Energy, AoI, Visibility, Distances) to balance short-term freshness and long-term sustainability.
- **Observed Behavior:**  
  Learns when **not** to transmit. Maintains stable battery levels while minimizing queue imbalance, resulting in consistently lower global AoI.
---

## üõ†Ô∏è Problem Architecture

### 1) Environment: `OCO2Env` (`env.py`)
A custom Gym environment that precomputes:
- **Physics-inspired Simulation**: Driven by **OCO-2 TLE orbit propagation (Skyfield)**.
- **Visibility Map**: Line-of-Sight (LoS) between OCO-2 and 10 Ground Stations.
- **Distance Matrix**: Dynamic distance (km) for Friis/FSPL energy modeling
- **State (55D/65D)**: Includes `minute index`, `AoI (LEO/GS)`, `queues (LEO/GS)`, `LoS flags`, `distance`, `normalized queue usage`, `battery`, and `optional future-LoS features`

### 2) Reward Design
The reward is a **weighted combination of AoI / queue / energy components**, plus explicit penalties (e.g., overflow, downlink when not visible, insufficient battery, transmission overhead).  
This encourages the agent to prioritize fresh data **without collapsing queues or draining energy**.

### 3) Planned Extension: Hierarchical DQN (h-DQN)
To improve **interpretability** and **fault isolation**, a planned direction is to decompose decisions:
- **High-level**: selects an operating mode (e.g., ‚Äúprioritize energy‚Äù vs ‚Äúprioritize freshness‚Äù)
- **Low-level**: selects a specific station conditioned on that mode

---

## üìÇ Repository Structure

```text
‚îú‚îÄ‚îÄ env.py                 # OCO2Env (Gym) & physics-inspired simulation.
‚îú‚îÄ‚îÄ satellite_utils.py     # Utilities for TLE handling and Line-of-Sight (LoS) calculations.
‚îú‚îÄ‚îÄ agent.py               # DQN (PER + n-step return).
‚îú‚îÄ‚îÄ main_all.py            # DQN Training
‚îú‚îÄ‚îÄ eval.py                # Integrated evaluation script for comparing all policies.
‚îú‚îÄ‚îÄ random_policy.py       # Heuristic policy selecting the nearest visible GS.
‚îî‚îÄ‚îÄ greedy_policy.py       # Baseline policy selecting actions randomly to test lower bounds.

```
---
## üöÄ Quick Start

### 1) Setup
```bash
python -m venv .venv
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
# source .venv/bin/activate

pip install -r requirements.txt
```
### 2) Train & Evaluate
```bash
# DQN training
python main_all.py

# Run evaluation using saved checkpoints
python eval.py
```
---
## üõ†Ô∏è Outputs & Artifacts

### 1) Training Outputs (`/checkpoints`)
After running `night_train.py`, the following files are generated:
* `dqn_agent_best.pt`: Model checkpoint with the highest episodic return.
* `dqn_agent_last.pt`: Final model checkpoint at the end of training.
* `train_progress.csv` & `episode_rewards.csv`: Full training logs.
* `curve_final.png`: Training reward visualization (updated periodically as `curve_epXXXX.png`).

### 2) Evaluation Outputs
Running `eval.py` produces comparative logs and visualizations:
* **Episode-level:** `random_log.csv`, `greedy_log.csv`, `dqn_best_log.csv` (Total rewards per episode).
* **Step-level:** `*_steps_log.csv` (Per-step energy levels and individual AoI for all 10 GS), `*_queue_log.csv` (Data backlog status for each GS queue).
* **Figures:** Comparative plots saved in the `figs/` directory.

---

## ‚úÖ Reproducibility Checklist

To ensure consistent results across different environments, this repository follows a strict reproducibility protocol:

* **Fixed Seeding:** `SEED = 42` is applied globally to both training and evaluation.
* **Deterministic Eval:** Evaluation runs with $\epsilon=0$ (no exploration) for DQN.
* **Environment Consistency:** Ensure the following parameters remain identical:
  - TLE lines & Ground Station coordinates.
  - Simulation `start_date` and total cycles.
  - Queue capacities and energy consumption parameters.

---

## üìå Technical Notes

* **Simulation Nature:** This is a **physics-based simulation** (orbit/visibility/distance) driven by TLE data, not a proprietary downlink dataset.
* **Customization:** If you wish to publish results based on this code, please document your exact reward weights ($w_n$) and environment configurations.
---
## üìÑ Reference
* **Paper:** [Energy-Efficient Ground Station Optimization for LEO Satellites via Reinforcement Learning](https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE12132784)
* **Conference:** KICS 2025 (Oral Session)
