# ğŸ›°ï¸ Energy-Efficient Ground Station Selection for LEO Downlink  
### RL Baselines: Random / Greedy / DQN (TLE-driven OCO-2 Simulation)

[![KICS 2025](https://img.shields.io/badge/Paper-KICS%202025-blue)](https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE12132784)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-green.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository implements a **minute-level LEO downlink scheduling simulator** using **Skyfield + TLE (OCO-2 orbit)** and compares baseline policies (**Random / Distance-Greedy / DQN**) for a multi-objective decision problem balancing:

- **Freshness** via **AoI (Age of Information)**
- **Queue stability** under finite capacity and overflow risk
- **Energy sustainability** with **distance-dependent transmission cost**

> **Core question:** Given time-varying visibility and distance-dependent transmission cost, which ground station should a satellite downlink to at each minute to balance freshness (AoI), queue stability, and energy sustainability?

---

## ğŸ“Š Key Results (Evaluation)

Evaluation uses **pure exploitation** for DQN (**Îµ=0**), run over **N evaluation episodes** with identical environment settings.

> **Reproducibility note:** All metrics below are computed from CSV logs generated by `eval_only.py`.  
> See: `random_log.csv`, `greedy_log.csv`, `dqn_best_log.csv` and step-level logs (`*_steps_log.csv`, `*_queue_log.csv`).

| Metric | **Random** | **Distance-Greedy** | **DQN (Ours)** | Unit / Notes |
|:---|:---:|:---:|:---:|:---|
| **Mean Return** | -124.5 Â± 12.1 | -85.2 Â± 8.4 | **-42.8 Â± 3.2** | Higher is better |
| **Avg. AoI (GS)** | 145.2 | 98.4 | **62.1** | Minutes (lower is better) |
| **Avg. Battery Level** | 0.85 | 0.62 | **0.78** | Normalized (higher is better) |
| **Queue Dispersion** | 12.4 | 8.1 | **4.2** | Std across 10 GS (lower is better) |

**How to interpret this table**
- **Return** reflects the reward function used in the simulator (AoI, queue terms, energy term, plus penalties such as overflow / infeasible downlink / transmit overhead).
- **AoI / Queue Dispersion** are â€œbehavioralâ€ metrics that make the learned policy easier to justify than return alone.
- **Energy** is reported as a proxy via average battery level (normalized).


---

## ğŸ› ï¸ Problem Architecture

### 1) Environment: `OCO2Env` (`env.py`)
A custom Gym environment that precomputes:
- **Physics-inspired Simulation**: Driven by **OCO-2 TLE orbit propagation (Skyfield)**.
- **Visibility Map**: Line-of-Sight (LoS) between OCO-2 and 10 Ground Stations.
- **Distance Matrix**: Dynamic distance (km) for Friis/FSPL energy modeling
- **State (55D/65D)**: Includes `minute index`, `AoI (LEO/GS)`, `queues (LEO/GS)`, `LoS flags`, `distance`, `normalized queue usage`, `battery`, and `optional future-LoS features`

### 2) Reward Design
The reward is a **weighted combination of AoI / queue / energy components**, plus explicit penalties (e.g., overflow, downlink when not visible, insufficient battery, transmission overhead).  
This encourages the agent to prioritize fresh data **without collapsing queues or draining energy**.

### 3) Planned Extension: Hierarchical DQN (h-DQN)
To improve **interpretability** and **fault isolation**, a planned direction is to decompose decisions:
- **High-level**: selects an operating mode (e.g., â€œprioritize energyâ€ vs â€œprioritize freshnessâ€)
- **Low-level**: selects a specific station conditioned on that mode

---

## ğŸ“‚ Repository Structure

```text
â”œâ”€â”€ env.py                 # OCO2Env (Gym) & physics-inspired simulation
â”œâ”€â”€ agent.py               # DQN (PER + n-step return)
â”œâ”€â”€ night_train.py         # Time-limited training (checkpoints + curve snapshots)
â”œâ”€â”€ eval_only.py           # Eval-only rollouts + metrics export (CSV/PNG)
â”œâ”€â”€ random_policy.py       # Visibility-aware random baseline
â”œâ”€â”€ greedy_policy.py       # Closest visible station baseline
â””â”€â”€ checkpoints/           # Saved model checkpoints (.pt)

```
---
## ğŸš€ Quick Start

### 1) Setup
```bash
python -m venv .venv
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
# source .venv/bin/activate

pip install -r requirements.txt
```
### 2) Train & Evaluate
```bash
# Time-limited training with curve snapshots
python night_train.py

# Run evaluation using saved checkpoints
python eval_only.py
```
---
## ğŸ› ï¸ Outputs & Artifacts

### 1. Training Outputs (`/checkpoints`)
After running `night_train.py`, the following files are generated:
* `dqn_agent_best.pt`: Model checkpoint with the highest episodic return.
* `dqn_agent_last.pt`: Final model checkpoint at the end of training.
* `train_progress.csv` & `episode_rewards.csv`: Full training logs.
* `curve_final.png`: Training reward visualization (updated periodically as `curve_epXXXX.png`).

### 2. Evaluation Outputs (`/figs` & root)
Running `eval_only.py` produces comparative logs and visualizations:
* **Episode-level:** `random_log.csv`, `greedy_log.csv`, `dqn_best_log.csv`.
* **Step-level:** `*_steps_log.csv` (AoI + Energy dynamics), `*_queue_log.csv` (Per-GS queue status).
* **Figures:** Comparative plots saved in the `figs/` directory.

---

## âœ… Reproducibility Checklist

To ensure consistent results across different environments, this repository follows a strict reproducibility protocol:

* **Fixed Seeding:** `SEED = 42` is applied globally to both training and evaluation.
* **Deterministic Eval:** Evaluation runs with $\epsilon=0$ (no exploration) for DQN.
* **Environment Consistency:** Ensure the following parameters remain identical:
  - TLE lines & Ground Station coordinates.
  - Simulation `start_date` and total cycles.
  - Queue capacities and energy consumption parameters.
* **Statistical Significance:** Recommended to run **â‰¥30 eval episodes** to report stable mean/std metrics (10 episodes for quick sanity checks).

---

## ğŸ“Œ Technical Notes

* **Simulation Nature:** This is a **physics-based simulation** (orbit/visibility/distance) driven by TLE data, not a proprietary downlink dataset.
* **Customization:** If you wish to publish results based on this code, please document your exact reward weights ($w_n$) and environment configurations.
---
## ğŸ“„ Reference
* **Paper:** [Energy-Efficient Ground Station Optimization for LEO Satellites via Reinforcement Learning](https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE12132784)
* **Conference:** KICS 2025 (Oral Session)
